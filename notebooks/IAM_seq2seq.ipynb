{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "#from PIL import Image\n",
    "import random\n",
    "#import tensorflow as tf\n",
    "#import re\n",
    "#import datetime\n",
    "#import io\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "#import pickle\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from nltk.metrics.distance import edit_distance\n",
    "#import string\n",
    "#from utils import generate_token_index\n",
    "#from utils import score_prediction, generate_token_index, y_labels, generate_dataset\n",
    "import json\n",
    "import keras\n",
    "import string\n",
    "import yaml\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../configs/config.yml') as f:\n",
    "    config = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['images_folder'] = os.path.join('../', config['images_folder'])\n",
    "config['labels_file'] = os.path.join('../', config['labels_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['labels_file']) as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a fraction of the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only a fraction\n",
    "fraction = 0.1\n",
    "max_len = int(len(dataset['train']) * fraction)\n",
    "\n",
    "indices = np.random.randint(0, len(dataset['train']), max_len)\n",
    "dataset['train_subsampled'] = [dataset['train'][j] for j in indices]\n",
    "#len(dataset)\n",
    "#dataset = dataset_orig\n",
    "print(len(dataset['train_subsampled']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators.data_generator import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(config, dataset['train_subsampled'], shuffle=True, use_data_augmentation=False)\n",
    "val_generator = DataGenerator(config, dataset['val'], shuffle=True, use_data_augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[batch_x, batch_y1], batch_y2 = train_generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_size = config['image']['image_size']['y_size']\n",
    "x_size = config['image']['image_size']['x_size']\n",
    "max_seq_length = config['network']['max_seq_lenght']\n",
    "latent_dim = config['network']['latent_dim']\n",
    "num_decoder_tokens = train_generator.num_decoder_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense, TimeDistributed, Conv2D, MaxPooling2D, Reshape, Dropout, BatchNormalization, Activation, Bidirectional, concatenate, add, Lambda, Permute\n",
    "from keras.callbacks import EarlyStopping\n",
    "#import keras.backend as K\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#latent_dim = 512  # Latent dimensionality of the encoding space.\n",
    "#optimizer\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(y_size, x_size, 1), name='input_encoder')\n",
    "\n",
    "#encoder_inputs = Input(shape=(128, 384, 1), name='input_encoder')\n",
    "\n",
    "#encoder_inputs = Input(shape=(92, 248, 1), name='input_encoder')\n",
    "\n",
    "#(7,7)\n",
    "encoder_layer = Conv2D(16, (7, 7), strides=(1,1), padding='same', use_bias=False, name='conv_1')(encoder_inputs)\n",
    "encoder_layer = BatchNormalization(name='batch_norm_1')(encoder_layer)\n",
    "encoder_layer = Activation('relu', name='activation_1')(encoder_layer)\n",
    "encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_1')(encoder_layer)\n",
    "\n",
    "#(7,7)\n",
    "encoder_layer = Conv2D(32, (5, 5), padding='same', use_bias=False, name='conv_2')(encoder_layer)\n",
    "encoder_layer = BatchNormalization(name='batch_norm_2')(encoder_layer)\n",
    "encoder_layer = Activation('relu', name='activation_2')(encoder_layer)\n",
    "encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_2')(encoder_layer)\n",
    "\n",
    "#(5,5)\n",
    "encoder_layer = Conv2D(64, (5, 5), padding='same', use_bias=False, name='conv_3')(encoder_layer)\n",
    "encoder_layer = BatchNormalization(name='batch_norm_3')(encoder_layer)\n",
    "encoder_layer = Activation('relu', name='activation_3')(encoder_layer)\n",
    "encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_3')(encoder_layer)\n",
    "\n",
    "#(5,5)\n",
    "encoder_layer = Conv2D(128, (3, 3), padding='same', use_bias=False, name='conv_4')(encoder_layer)\n",
    "encoder_layer = BatchNormalization(name='batch_norm_4')(encoder_layer)\n",
    "encoder_layer = Activation('relu', name='activation_4')(encoder_layer)\n",
    "encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_4')(encoder_layer)\n",
    "\n",
    "#(3,3)\n",
    "encoder_layer = Conv2D(256, (3, 3), padding='same', use_bias=False, name='conv_5')(encoder_layer)\n",
    "encoder_layer = BatchNormalization(name='batch_norm_5')(encoder_layer)\n",
    "encoder_layer = Activation('relu', name='activation_5')(encoder_layer)\n",
    "encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_5')(encoder_layer)\n",
    "\n",
    "#(3,3)\n",
    "encoder_layer = Conv2D(512, (3, 3), padding='same', use_bias=False, name='conv_6')(encoder_layer)\n",
    "encoder_layer = BatchNormalization(name='batch_norm_6')(encoder_layer)\n",
    "encoder_layer = Activation('relu', name='activation_6')(encoder_layer)\n",
    "encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_6')(encoder_layer)\n",
    "\n",
    "conv_shapes = encoder_layer.shape[1:]\n",
    "timesteps = int(conv_shapes[0]*conv_shapes[1])\n",
    "num_features = int(conv_shapes[2])\n",
    "\n",
    "encoder_layer = Reshape((-1, num_features), name='reshape')(encoder_layer)\n",
    "\n",
    "#encoder\n",
    "encoder = LSTM(latent_dim, return_state=True, name='lstm_encoder')\n",
    "_, state_h, state_c = encoder(encoder_layer)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#decoder\n",
    "decoder_inputs = Input(shape=(max_seq_length, num_decoder_tokens), name='input_decoder_teacher_forcing')\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='lstm_decoder')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "#decoder_dense_time_dist = TimeDistributed(Dense(num_decoder_tokens, activation='softmax'), name='time_distributed_dense')\n",
    "#decoder_outputs = decoder_dense_time_dist(decoder_outputs)\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = True\n",
    "\n",
    "#batch_size = 128  # Batch size for training.\n",
    "epochs = 50 # Number of epochs to train for.\n",
    "\n",
    "# Early stopping  \n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "if train == True:    \n",
    "    # Run training\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "    #model.fit_generator(generator = generator epochs=epochs, verbose=1)\n",
    "\n",
    "    model.fit_generator(generator=train_generator, validation_data=val_generator, epochs=epochs, verbose=1, \n",
    "                        callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train == True:\n",
    "    model_json = model.to_json()\n",
    "    with open(\"../snapshots/graph.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"../snapshots/weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path = '../snapshots/graph.json'\n",
    "weights_path = '../snapshots/weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "#model = load_model(\"../snapshots/full_model.h5\")\n",
    "with open(graph_path, 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = '../datasets/words/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = DataGenerator(config, dataset['test'], shuffle=False, use_data_augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change images folder since you are in \"notebooks\" subfolder\n",
    "#test_generator.images_folder = '../datasets/words/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_test, labels_test = test_generator.get_full_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = random.randint(0, images_test.shape[0] - 1)\n",
    "plt.imshow(np.reshape(images_test[index, :, :,:], (test_generator.y_size, test_generator.x_size)), cmap=plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "print(\"label: \", labels_test[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inference models \n",
    "#encoder\n",
    "#encoder_inputs_inference = Input(shape=(x_size, y_size, 1), name='input_encoder_inference')\n",
    "encoder_inference = Model(model.get_input_at(0)[0], model.get_layer(\"lstm_encoder\").output[1:])\n",
    "\n",
    "#decoder\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_inputs_inference = Input(shape=(1, num_decoder_tokens), name='input_decoder_inference')\n",
    "\n",
    "decoder_lstm = model.get_layer(\"lstm_decoder\")\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_inference, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "#dense layer\n",
    "decoder_dense = model.get_layer('dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "#inference\n",
    "decoder_inference = Model([decoder_inputs_inference] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverse_target_char_index = train_generator.reverse_token_indices\n",
    "#token_indices = train_generator.token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_char_index = test_generator.reverse_token_indices\n",
    "token_indices = test_generator.token_indices\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq, max_decoder_seq_length=10):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_inference.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, token_indices['[']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "#    while not stop_condition:\n",
    "    i = 0\n",
    "    while i < max_decoder_seq_length:\n",
    "        i += 1\n",
    "        output_tokens, h, c = decoder_inference.predict( [target_seq] + states_value )\n",
    "           \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        token_value = np.max(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == ']' or token_value == 0.: \n",
    "            #or len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_test = []\n",
    "\n",
    "for ind in range(len(labels_test)):    \n",
    "#for ind in range(3):    \n",
    "\n",
    "    #row_pred = []\n",
    "\n",
    "    input_seq = np.reshape( images_test[ind,:,:], (-1, y_size,  x_size, 1) )\n",
    "    \n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    decoded_sentence = decoded_sentence.replace(\"]\", \"\")\n",
    "    \n",
    "    #row_pred.append(words_test[ind])\n",
    "    #row_pred.append(decoded_sentence)\n",
    "    \n",
    "    pred_test.append(decoded_sentence)\n",
    "    \n",
    "    #print('Data', words_test[ind], '-', ind, 'out of', len(words_test))\n",
    "    print('GT:', labels_test[ind])\n",
    "    print('Decoded:', decoded_sentence, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, batch_dim, num_decoder_tokens, max_decoder_seq_length, token_indices, reverse_token_indices):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_inference.predict(input_seq, batch_size = batch_dim)\n",
    "\n",
    "    full_seq = np.zeros((batch_dim, 1, num_decoder_tokens))\n",
    "\n",
    "    target_seq = np.zeros((batch_dim, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[:, token_indices['[']] = 1.\n",
    "   \n",
    "    for i in range(max_decoder_seq_length):\n",
    "        output_tokens, h, c = decoder_inference.predict([np.expand_dims(target_seq, axis=1)] + states_value, \n",
    "                                                        batch_size = batch_dim )\n",
    "    \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[:, -1, :], axis=1)\n",
    "        #        confidence.append(np.max(output_tokens[:, -1, :]))\n",
    "\n",
    "        target_seq = np.zeros((batch_dim, num_decoder_tokens))    \n",
    "        for j in range(batch_dim):\n",
    "            target_seq[j, sampled_token_index[j]] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "        #concatenate with the full sequence array\n",
    "        full_seq = np.concatenate((full_seq, np.expand_dims(target_seq, axis=1)), axis=1)\n",
    "\n",
    "    #remove first time element (is empty)\n",
    "    full_seq = full_seq[:, 1:, :]\n",
    "    decoded_sentences = []\n",
    "    \n",
    "    for i in range(batch_dim):\n",
    "        sentence = []\n",
    "        for j in range(full_seq.shape[1]):\n",
    "            sampled_token_index = np.argmax(full_seq[i, j, :])   \n",
    "            sentence.append(reverse_token_indices[sampled_token_index])\n",
    "            #print(sampled_token_index)\n",
    "    \n",
    "        decoded_sentences.append(''.join(sentence).replace(\"]\", \"\"))  \n",
    "            \n",
    "    return decoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_generator.reverse_token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(images, batch_size = 512):\n",
    "    n_images = images.shape[0]\n",
    "    y_size = images.shape[1]\n",
    "    x_size = images.shape[2]\n",
    "\n",
    "    n_batches = (n_images + batch_size - 1) // batch_size\n",
    "    \n",
    "    output_list = []\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "\n",
    "#    for i in range(1):\n",
    "\n",
    "        batch_in, batch_out = (batch_size)* i, (batch_size)* i + batch_size\n",
    "\n",
    "        if batch_out >= n_images:\n",
    "            batch_out = n_images\n",
    "\n",
    "        input_seq = images[batch_in:batch_out, :, :, :]\n",
    "        batch_dim = batch_out - batch_in\n",
    "        decoded_sentences = decode_sequence(input_seq, batch_dim, test_generator.num_decoder_tokens, \n",
    "                                          test_generator.max_seq_length,\n",
    "                                          test_generator.token_indices,\n",
    "                                          test_generator.reverse_token_indices)\n",
    "\n",
    "        output_list.append(decoded_sentences)\n",
    "    \n",
    "    #flatten list\n",
    "    flattened_list = [item for sublist in output_list for item in sublist]\n",
    "    \n",
    "    return flattened_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predict(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_prediction(y_true, y_pred):\n",
    "    words_identified = 0\n",
    "    characters_identified = 0\n",
    "    char_tot = 0\n",
    "#    CER = 0\n",
    "\n",
    "#    list_accuracy_characters = []\n",
    "    \n",
    "    for i in range(len(y_pred)):\n",
    "        #pred_row = [y_true[i], y_pred[i]] \n",
    "        \n",
    "        #check if date are the same\n",
    "        #if pred_row[0] == pred_row[1]:\n",
    "        if y_true[i] == y_pred[i]:\n",
    "\n",
    "            words_identified += 1\n",
    "            \n",
    "#        if len(pred_row[1]) < len(pred_row[0]):\n",
    "#            pred_row[1] += '-' * (len(pred_row[0]) - len(pred_row[1]))    \n",
    "#        elif len(pred_row[1]) > len(pred_row[1]):\n",
    "\n",
    "#            pred_row[1] = pred_row[1][0:len(pred_row[0])]\n",
    "\n",
    "        #check the number of characters that are the same\n",
    " #       print(y_true[i])\n",
    " #       print(y_pred[i])\n",
    "        \n",
    "        levenshtein_distance = edit_distance(y_true[i], y_pred[i])\n",
    "        n_char = np.maximum(len(y_true[i]), len(y_pred[i]))\n",
    "        \n",
    "        normalized_distance = levenshtein_distance/n_char\n",
    "\n",
    "        characters_identified += normalized_distance\n",
    "#        char_tot += n_char\n",
    "        \n",
    "#        CER += normalized_distance\n",
    "        \n",
    "#        print(len(y_true[i]))\n",
    "#        for k in range(len(y_true[i])):\n",
    "#            print()\n",
    "#            if y_true[i][k] == y_pred[1][k]:\n",
    "#        characters_identified += 1\n",
    "#        char_tot += 1\n",
    "\n",
    "    # array_accuracy_characters = np.asarray(list_accuracy_characters)\n",
    "    CER = float((characters_identified) / len(y_true))\n",
    "    WER = (len(y_pred) - words_identified)/len(y_pred) \n",
    "        \n",
    "    return CER, WER\n",
    "#    return WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CER_test, WER_test = score_prediction(labels_test, pred_test)\n",
    "#WER_test = score_prediction(words_test, pred_test)\n",
    "print('CER: ', round(CER_test * 100, 3), '%')\n",
    "print('WER: ', round(WER_test * 100, 3), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------- Old code---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load filenames and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_folder + 'gt.json') as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_orig = dataset['train']\n",
    "dataset_val = dataset['val']\n",
    "dataset_test = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only a subsample for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only a fraction\n",
    "fraction = 0.1\n",
    "max_len = int(len(dataset_train_orig) * fraction)\n",
    "\n",
    "indices = np.random.randint(0, len(dataset_train_orig), max_len)\n",
    "dataset_train = [dataset_train_orig[j] for j in indices]\n",
    "#len(dataset)\n",
    "#dataset = dataset_orig\n",
    "print(len(dataset_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(dataset_train, 128, 384, 1, batch_size = 64)\n",
    "val_generator = DataGenerator(dataset_val, 128, 384, 1, batch_size = 64)\n",
    "\n",
    "[batch_x, batch_y1], batch_y2 = train_generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " batch_y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_size = train_generator.y_size\n",
    "x_size = train_generator.x_size\n",
    "max_decoder_seq_length = train_generator.max_decoder_seq_length\n",
    "num_decoder_tokens = train_generator.num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator.max_decoder_seq_length = max_decoder_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator.max_decoder_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = random.randint(0, batch_x.shape[0] - 1)\n",
    "plt.figure(figsize=(9, 9)) \n",
    "#plt.imshow(np.reshape(array_images[index, :, :], (y_size, x_size)), cmap=plt.get_cmap('gray'))\n",
    "plt.imshow(np.reshape(batch_x[index], (128,384)), cmap=plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "#print(\"word = \", words[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense, TimeDistributed, Conv2D, MaxPooling2D, Reshape, Dropout, BatchNormalization, Activation, Bidirectional, concatenate, add, Lambda, Permute\n",
    "from keras.callbacks import EarlyStopping\n",
    "#import keras.backend as K\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 512  # Latent dimensionality of the encoding space.\n",
    "#optimizer\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(y_size, x_size, 1), name='input_encoder')\n",
    "\n",
    "#encoder_inputs = Input(shape=(128, 384, 1), name='input_encoder')\n",
    "\n",
    "#encoder_inputs = Input(shape=(92, 248, 1), name='input_encoder')\n",
    "\n",
    "#(7,7)\n",
    "encoder_layer = Conv2D(16, (7, 7), strides=(1,1), padding='same', use_bias=False, name='conv_1')(encoder_inputs)\n",
    "encoder_layer = BatchNormalization(name='batch_norm_1')(encoder_layer)\n",
    "encoder_layer = Activation('relu', name='activation_1')(encoder_layer)\n",
    "encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_1')(encoder_layer)\n",
    "\n",
    "#(7,7)\n",
    "encoder_layer = Conv2D(32, (5, 5), padding='same', use_bias=False, name='conv_2')(encoder_layer)\n",
    "encoder_layer = BatchNormalization(name='batch_norm_2')(encoder_layer)\n",
    "encoder_layer = Activation('relu', name='activation_2')(encoder_layer)\n",
    "encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_2')(encoder_layer)\n",
    "\n",
    "#(5,5)\n",
    "encoder_layer = Conv2D(64, (5, 5), padding='same', use_bias=False, name='conv_3')(encoder_layer)\n",
    "encoder_layer = BatchNormalization(name='batch_norm_3')(encoder_layer)\n",
    "encoder_layer = Activation('relu', name='activation_3')(encoder_layer)\n",
    "encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_3')(encoder_layer)\n",
    "\n",
    "#(5,5)\n",
    "encoder_layer = Conv2D(128, (3, 3), padding='same', use_bias=False, name='conv_4')(encoder_layer)\n",
    "encoder_layer = BatchNormalization(name='batch_norm_4')(encoder_layer)\n",
    "encoder_layer = Activation('relu', name='activation_4')(encoder_layer)\n",
    "encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_4')(encoder_layer)\n",
    "\n",
    "#(3,3)\n",
    "encoder_layer = Conv2D(256, (3, 3), padding='same', use_bias=False, name='conv_5')(encoder_layer)\n",
    "encoder_layer = BatchNormalization(name='batch_norm_5')(encoder_layer)\n",
    "encoder_layer = Activation('relu', name='activation_5')(encoder_layer)\n",
    "encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_5')(encoder_layer)\n",
    "\n",
    "#(3,3)\n",
    "encoder_layer = Conv2D(512, (3, 3), padding='same', use_bias=False, name='conv_6')(encoder_layer)\n",
    "encoder_layer = BatchNormalization(name='batch_norm_6')(encoder_layer)\n",
    "encoder_layer = Activation('relu', name='activation_6')(encoder_layer)\n",
    "encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_6')(encoder_layer)\n",
    "\n",
    "\n",
    "#(2,2)\n",
    "#encoder_layer = Conv2D(512, (3, 3), padding='same', use_bias=False, name='conv_6')(encoder_layer)\n",
    "#encoder_layer = BatchNormalization(name='batch_norm_6')(encoder_layer)\n",
    "#encoder_layer = Activation('relu', name='activation_6')(encoder_layer)\n",
    "#encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_6')(encoder_layer)\n",
    "\n",
    "\n",
    "#(3,3)\n",
    "#encoder_layer = Conv2D(512, (3, 3), padding='same', use_bias=False, name='conv_6')(encoder_layer)\n",
    "#encoder_layer = BatchNormalization(name='batch_norm_6')(encoder_layer)\n",
    "#encoder_layer = Activation('relu', name='activation_6')(encoder_layer)\n",
    "#encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_6')(encoder_layer)\n",
    "\n",
    "conv_shapes = encoder_layer.shape[1:]\n",
    "timesteps = int(conv_shapes[0]*conv_shapes[1])\n",
    "num_features = int(conv_shapes[2])\n",
    "\n",
    "encoder_layer = Reshape((-1, num_features), name='reshape')(encoder_layer)\n",
    "\n",
    "#encoder\n",
    "encoder = LSTM(latent_dim, return_state=True, name='lstm_encoder')\n",
    "_, state_h, state_c = encoder(encoder_layer)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#decoder\n",
    "decoder_inputs = Input(shape=(max_decoder_seq_length, num_decoder_tokens), name='input_decoder_teacher_forcing')\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='lstm_decoder')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense_time_dist = TimeDistributed(Dense(num_decoder_tokens, activation='softmax'), name='time_distributed_dense')\n",
    "decoder_outputs = decoder_dense_time_dist(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = True\n",
    "\n",
    "#batch_size = 128  # Batch size for training.\n",
    "epochs = 50 # Number of epochs to train for.\n",
    "\n",
    "#n_times_aug = 1\n",
    "\n",
    "# Early stopping  \n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "if train == True:    \n",
    "    # Run training\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "#    model.fit([array_images_train, decoder_input_data_train], decoder_target_data_train, batch_size=batch_size,\n",
    "#              epochs=epochs, validation_data = ([array_images_val, decoder_input_data_val], decoder_target_data_val),\n",
    "#              verbose=1, callbacks=[])\n",
    "\n",
    "    #model.fit_generator(generator = generator epochs=epochs, verbose=1)\n",
    "\n",
    "    model.fit_generator(generator=train_generator, validation_data=val_generator, epochs=epochs, verbose=1, \n",
    "                        callbacks=[])\n",
    "\n",
    "#    model.fit_generator(generator=train_generator, epochs=epochs, verbose=1, \n",
    "#                        callbacks=[])\n",
    "\n",
    "    \n",
    "#    model.fit_generator(train_datagen.flow([array_images_train, decoder_input_data_train], decoder_target_data_train, batch_size=batch_size),\n",
    "#                        steps_per_epoch = n_times_aug * len(array_images_train) / batch_size, epochs=epochs, \n",
    "#                        validation_data = val_datagen.flow([array_images_val, decoder_input_data_val], decoder_target_data_val, batch_size=batch_size),\n",
    "#                        validation_steps =  1, verbose=1, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train == True:\n",
    "    model_json = model.to_json()\n",
    "    with open(\"../snapshots/graph.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"../snapshots/weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if train == False:\n",
    "#model.save('../snapshots/model_IAM.h5')\n",
    "#model.save_weights(\"../snapshots/weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train == False:\n",
    "    from keras.models import load_model, model_from_json\n",
    "    json_file = open('../snapshots/graph.json', 'r')\n",
    "    model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"../snapshots/weights.h5\")    \n",
    "#    model = load_model('../snapshots/model_IAM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(\"../snapshots/weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = '../datasets/words/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = DataGenerator(config, dataset['test'], shuffle=False, use_data_augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change images folder since you are in \"notebooks\" subfolder\n",
    "#test_generator.images_folder = '../datasets/words/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_test, labels_test = test_generator.get_full_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = random.randint(0, images_test.shape[0] - 1)\n",
    "plt.imshow(np.reshape(images_test[index, :, :,:], (test_generator.y_size, test_generator.x_size)), cmap=plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "print(\"label: \", labels_test[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inference models \n",
    "#encoder\n",
    "#encoder_inputs_inference = Input(shape=(x_size, y_size, 1), name='input_encoder_inference')\n",
    "encoder_inference = Model(model.get_input_at(0)[0], model.get_layer(\"lstm_encoder\").output[1:])\n",
    "\n",
    "#decoder\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_inputs_inference = Input(shape=(1, num_decoder_tokens), name='input_decoder_inference')\n",
    "\n",
    "decoder_lstm = model.get_layer(\"lstm_decoder\")\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_inference, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "#dense layer\n",
    "decoder_dense = model.get_layer('time_distributed_dense').layer\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "#inference\n",
    "decoder_inference = Model([decoder_inputs_inference] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_char_index = train_generator.reverse_token_indices\n",
    "token_indices = train_generator.token_indices\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq, max_decoder_seq_length=5):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_inference.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, token_indices['[']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_inference.predict( [target_seq] + states_value )\n",
    "           \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == ']'): \n",
    "            #or len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_test = images_test[0:1000, ...]\n",
    "labels_test = labels_test[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_test = []\n",
    "\n",
    "for ind in range(len(labels_test)):    \n",
    "    #row_pred = []\n",
    "\n",
    "    input_seq = np.reshape( images_test[ind,:,:], (-1, y_size,  x_size, 1) )\n",
    "    \n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    decoded_sentence = decoded_sentence.replace(\"]\", \"\")\n",
    "    \n",
    "    #row_pred.append(words_test[ind])\n",
    "    #row_pred.append(decoded_sentence)\n",
    "    \n",
    "    pred_test.append(decoded_sentence)\n",
    "    \n",
    "    #print('Data', words_test[ind], '-', ind, 'out of', len(words_test))\n",
    "    print('GT:', labels_test[ind])\n",
    "    print('Decoded:', decoded_sentence, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_prediction(y_true, y_pred):\n",
    "    words_identified = 0\n",
    "    characters_identified = 0\n",
    "    char_tot = 0\n",
    "#    CER = 0\n",
    "\n",
    "#    list_accuracy_characters = []\n",
    "    \n",
    "    for i in range(len(y_pred)):\n",
    "        #pred_row = [y_true[i], y_pred[i]] \n",
    "        \n",
    "        #check if date are the same\n",
    "        #if pred_row[0] == pred_row[1]:\n",
    "        if y_true[i] == y_pred[i]:\n",
    "\n",
    "            words_identified += 1\n",
    "            \n",
    "#        if len(pred_row[1]) < len(pred_row[0]):\n",
    "#            pred_row[1] += '-' * (len(pred_row[0]) - len(pred_row[1]))    \n",
    "#        elif len(pred_row[1]) > len(pred_row[1]):\n",
    "\n",
    "#            pred_row[1] = pred_row[1][0:len(pred_row[0])]\n",
    "\n",
    "        #check the number of characters that are the same\n",
    " #       print(y_true[i])\n",
    " #       print(y_pred[i])\n",
    "        \n",
    "        levenshtein_distance = edit_distance(y_true[i], y_pred[i])\n",
    "        n_char = np.maximum(len(y_true[i]), len(y_pred[i]))\n",
    "        \n",
    "        normalized_distance = levenshtein_distance/n_char\n",
    "\n",
    "        characters_identified += normalized_distance\n",
    "#        char_tot += n_char\n",
    "        \n",
    "#        CER += normalized_distance\n",
    "        \n",
    "#        print(len(y_true[i]))\n",
    "#        for k in range(len(y_true[i])):\n",
    "#            print()\n",
    "#            if y_true[i][k] == y_pred[1][k]:\n",
    "#        characters_identified += 1\n",
    "#        char_tot += 1\n",
    "\n",
    "    # array_accuracy_characters = np.asarray(list_accuracy_characters)\n",
    "    CER = float((characters_identified) / len(y_true))\n",
    "    WER = (len(y_pred) - words_identified)/len(y_pred) \n",
    "        \n",
    "    return CER, WER\n",
    "#    return WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CER_test, WER_test = score_prediction(labels_test, pred_test)\n",
    "#WER_test = score_prediction(words_test, pred_test)\n",
    "print('CER: ', round(CER_test * 100, 3), '%')\n",
    "print('WER: ', round(WER_test * 100, 3), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CER_train, WER_train = score_prediction(words_train, pred_train)\n",
    "##print('CER: ', round(CER_train * 100, 3), '%')\n",
    "#print('CER: ', round(CER_train * 100, 3), '%')\n",
    "#print('WER: ', round(WER_train * 100, 3), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
